---
title: "Project 10"
output:
  pdf_document: 
    extra_dependencies: ['amsmath']
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Problem 29

```{r}
library(caret)
library(glmnet)
library(pROC)
```

```{r}
load(file='yeastStorey.rda')
```

```{r}
print(paste("Number of samples (N):", nrow(data)))
print(paste("Number of features (p):", ncol(data)))
```

## Splitting data into training and testing subsets

```{r}
set.seed(42)
trainIndex <- createDataPartition(data$Marker, p=0.7, list=FALSE, times=1)
trainData <- data[trainIndex,]
testData <- data[-trainIndex,]
```

## Cross-validation of elastic-net model

```{r}
# Preparing data for cv.glmnet
x <- trainData[, !(names(trainData) %in% c("Marker"))]
x <- as.matrix(x)
y <- trainData$Marker
```

```{r}
# Executing 10-fold CV for each value of alpha
foldid <- sample(1:10, size=length(y), replace=TRUE)
alphas <- seq(0, 1, by=0.1)

elasticNetCVAlpha <- function(alpha) {
  cv.glmnet(x, y, family="binomial", alpha=alpha, nfolds=10, foldid=foldid)
}

resultsCV <- lapply(alphas, elasticNetCVAlpha)
```

```{r}
# Finding the optimal alpha
minMeanCVMIdx <- 1
minMeanCVM <- mean(resultsCV[[1]]$cvm)
for(i in 1:length(alphas)) { 
  if(minMeanCVM > mean(resultsCV[[i]]$cvm)) { 
    minMeanCVM <- mean(resultsCV[[i]]$cvm)
    minMeanCVMIdx <- i
  }
  # Reporting mean of mean cross-validated error of each alpha
  print(paste0("alpha=", alphas[i], "; error=", mean(resultsCV[[i]]$cvm))) 
}
```
### Finding optimal alpha

$\alpha$ with which mean of mean cross-validated error is the smallest: 
$\alpha=$ `r alphas[minMeanCVMIdx]`.
This $\alpha$ will be considered as optimal.

```{r}
print(paste("Min. mean of mean cross-validated error:", minMeanCVM))
optimalAlphaIdx <- minMeanCVMIdx
optimalAlpha <- alphas[optimalAlphaIdx]
```

### Plotting mean cross-validated error

Cross-validated error function is binomial deviance. The plot of $log(\lambda)$
versus mean cross-validated error is done using results retrieved with 
$\alpha=$ `r optimalAlpha`.

```{r}
plot(resultsCV[[optimalAlphaIdx]], ylab="mean cross-validated error")
```

### Plotting trace curve of coefficients

The plot of $log(\lambda)$ versus coefficients is done using results retrieved 
with $\alpha=$ `r optimalAlpha`.

```{r}
plot(resultsCV[[optimalAlphaIdx]]$glmnet.fit, "lambda")
```

### Picking optimal lambda

```{r}
optimalLambdaIdx <- which.min(resultsCV[[optimalAlphaIdx]]$cvm)
optimalLambda <- resultsCV[[optimalAlphaIdx]]$lambda[[optimalLambdaIdx]]
```

Optimal $\lambda=$ `r optimalLambda`.

## Fitting model on the whole training set

```{r}
trainedModel <- glmnet(x, y, family="binomial", alpha=optimalAlpha, lambda=optimalLambda)
trainedModel
```

## Testing model

```{r}
# Preparing data for inference using fitted glmnet
xTest <- testData[, !(names(testData) %in% c("Marker"))]
xTest <- as.matrix(xTest)
yTest <- testData$Marker
```

```{r}
# Making predictions and evaluating performance
predictions <- predict(trainedModel, newx=xTest)
resultsTest <- assess.glmnet(predictions, newy=yTest, family="binomial")

roc(yTest, predictions, plot=TRUE)
```

```{r, eval=FALSE}
library(rmarkdown)
render("project10.Rmd", pdf_document(TRUE), "Indilewitsch_Toidze_Houhamdi_Pudziuvelyte_Project10.pdf")
```
